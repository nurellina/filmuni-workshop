{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Scraping with Selenium and Chrome webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple notebook to work with Selenium and Chrome Webdriver in order to scrape a dynamic website. While for static websites we can use [Beautiful Soup](https://pypi.org/project/beautifulsoup4/), when we are working with dynamic ones we need Selenium. The key aspect of [Selenium](https://pypi.org/project/selenium/) is that it executes javascript when loading a page request, this allows for dynamic websites to be scraped easily. To complete our exercise we will also use [pandas](https://pypi.org/project/pandas/), the data science and data analysis library, in order to create a dataframe and export our data to our local machine.\n",
    "\n",
    "Tutorial sections:\n",
    "- **Headless browsers**: Start a WebDriver session with Selenium\n",
    "- **One page simple scraping**: retrieve all the content from one page and create a dataframe\n",
    "- **Scraping across pages**: Continously load new content, then create a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will make scraping possible\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "\n",
    "# Will allow to request, access and write urls of images and other links\n",
    "import urllib.request\n",
    "import requests\n",
    "\n",
    "# Will provide structure to data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Selenium and driver instance using Chrome WebDriver. To properly install ChromeDriver you can manually set the path following [this guide](https://www.kenst.com/2015/03/including-the-chromedriver-location-in-macos-system-path/) or use `pip install webdriver-manager` to manage it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: start Chrome webdriver session**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this exercise we will get some movies from [Rotten Tomatoes](https://www.rottentomatoes.com/browse/dvd-streaming-all). We are interested in creating a dataset of titles with their ranking, date of release and cover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Current google-chrome version is 87.0.4280\n",
      "[WDM] - Get LATEST driver version for 87.0.4280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Driver [/Users/francescamorini/.wdm/drivers/chromedriver/mac64/87.0.4280.88/chromedriver] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# We start an headless Chrome session. (Headless means that we don't open a visual browser window)\n",
    "options = Options()\n",
    "options.headless = True\n",
    "options.add_argument(\"--window-size=1920,1200\")\n",
    "\n",
    "# instead of setting path manually (still possible but a bit annoying) we use DriverManager\n",
    "driver = webdriver.Chrome(executable_path=ChromeDriverManager().install(), options=options)\n",
    "driver.get(\"https://www.rottentomatoes.com/browse/dvd-streaming-all\")\n",
    "\n",
    "# print(driver.page_source)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One time simple scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept behind scraping is quite easy to grasp. We are creating a script that behaves like a human. By using the correct methods we can fake human interactions on webpage. While usually scripts \"see\" different things than us, Selenium, Beautiful Soup and WebDrivers are built to see pages as we would do. Scripts are obviously faster, they don't get to go to the toilet and they do everything very enthusiastically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Get movies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only getting first page results\n",
    "movies = driver.find_elements_by_class_name('mb-movie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: loop through movies and build a list of lists containing our data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "[['Parallel', ' 80%', 'Available Dec 11', 'data/img/poster-1.jpg'], ['The Emoji Story (Picture Character)', ' 91%', 'Available Dec 22', 'data/img/poster-2.jpg'], ['Yellow Rose', ' 86%', 'Available Jan 5', 'data/img/poster-3.jpg'], ['Soul', ' 96%', 'Available Dec 25', 'data/img/poster-4.jpg'], ['Sing Me A Song', ' 88%', 'Available Jan 1', 'data/img/poster-5.jpg'], ['Pieces Of A Woman', ' 77%', 'Available Jan 7', 'data/img/poster-6.jpg'], ['Lupin III: The First', ' 94%', 'Available Dec 15', 'data/img/poster-7.jpg'], [\"Sylvie'S Love\", ' 92%', 'Available Dec 23', 'data/img/poster-8.jpg'], ['Beasts Clawing At Straws', ' 96%', 'Available Dec 15', 'data/img/poster-9.jpg'], ['Shadow In The Cloud', ' 76%', 'Available Jan 1', 'data/img/poster-10.jpg'], ['I Am Lisa', ' 91%', 'Available Jan 5', 'data/img/poster-11.jpg'], ['Shortcut', ' 58%', 'Available Dec 22', 'data/img/poster-12.jpg'], ['The Midnight Sky', ' 51%', 'Available Dec 23', 'data/img/poster-13.jpg'], [\"Ma Rainey'S Black Bottom\", ' 98%', 'Available Dec 18', 'data/img/poster-14.jpg'], ['Archenemy', ' 71%', 'Available Dec 11', 'data/img/poster-15.jpg'], ['Skylines', ' 52%', 'Available Dec 18', 'data/img/poster-16.jpg'], ['Hunter Hunter', ' 93%', 'Available Dec 18', 'data/img/poster-17.jpg'], ['The Prom', ' 57%', 'Available Dec 11', 'data/img/poster-18.jpg'], ['Wild Mountain Thyme', ' 27%', 'Available Dec 11', 'data/img/poster-19.jpg'], ['The Stand In', ' 29%', 'Available Dec 11', 'data/img/poster-20.jpg'], ['Sister Of The Groom', ' 47%', 'Available Dec 18', 'data/img/poster-21.jpg'], ['Max Cloud', ' 50%', 'Available Dec 18', 'data/img/poster-22.jpg'], ['Breach', ' 20%', 'Available Dec 18', 'data/img/poster-23.jpg'], ['Ip Man: Kung Fu Master', ' 44%', 'Available Dec 11', 'data/img/poster-24.jpg'], ['We Can Be Heroes', ' 71%', 'Available Jan 1', 'data/img/poster-25.jpg'], ['Redemption Day', ' 6%', 'Available Jan 12', 'data/img/poster-26.jpg'], ['Yearly Departed', ' 80%', 'Available Dec 30', 'data/img/poster-27.jpg'], ['Modern Persuasion', ' 50%', 'Available Dec 18', 'data/img/poster-28.jpg'], ['The One You Feed', ' 40%', 'Available Dec 29', 'data/img/poster-29.jpg'], ['Stars Fell On Alabama', ' 63%', 'Available Jan 8', 'data/img/poster-30.jpg'], ['If Not Now, When?', ' 43%', 'Available Jan 8', 'data/img/poster-31.jpg'], ['Ariana Grande: Excuse Me, I Love You', ' 50%', 'Available Dec 21', 'data/img/poster-32.jpg'], ['Princess Of The Row', ' 88%', 'Available Nov 27', 'data/img/poster-33.jpg'], ['Hearts And Bones', ' 81%', 'Available Nov 20', 'data/img/poster-34.jpg'], ['Collective (Colectiv)', ' 99%', 'Available Nov 20', 'data/img/poster-35.jpg'], ['Billie', ' 97%', 'Available Dec 4', 'data/img/poster-36.jpg'], ['The Giant', ' 50%', 'Available Nov 13', 'data/img/poster-37.jpg'], ['Iron Mask', ' 21%', 'Available Nov 20', 'data/img/poster-38.jpg'], ['18 To Party', ' 57%', 'Available Dec 1', 'data/img/poster-39.jpg'], ['The Nest', ' 88%', 'Available Nov 17', 'data/img/poster-40.jpg'], ['Luxor', ' 91%', 'Available Dec 4', 'data/img/poster-41.jpg'], ['Beast Mode', ' 57%', 'Available Dec 1', 'data/img/poster-42.jpg'], ['Black Bear', ' 87%', 'Available Dec 4', 'data/img/poster-43.jpg'], ['Train To Busan Presents: Peninsula', ' 53%', 'Available Nov 24', 'data/img/poster-44.jpg'], ['Lie Exposed', ' 0%', 'Available Nov 20', 'data/img/poster-45.jpg'], ['The Donut King', ' 97%', 'Available Nov 20', 'data/img/poster-46.jpg'], ['Stardust', ' 17%', 'Available Nov 25', 'data/img/poster-47.jpg'], ['Survival Skills', ' 85%', 'Available Dec 4', 'data/img/poster-48.jpg'], ['The Walrus And The Whistleblower', ' 40%', 'Available Nov 24', 'data/img/poster-49.jpg'], ['Ammonite', ' 67%', 'Available Dec 4', 'data/img/poster-50.jpg'], ['Run', ' 90%', 'Available Nov 20', 'data/img/poster-51.jpg'], [\"A Chef'S Voyage\", ' 67%', 'Available Nov 24', 'data/img/poster-52.jpg'], ['Minor Premise', ' 92%', 'Available Dec 4', 'data/img/poster-53.jpg'], ['Chick Fight', ' 36%', 'Available Nov 13', 'data/img/poster-54.jpg'], ['Echo Boomers', ' 38%', 'Available Nov 13', 'data/img/poster-55.jpg'], ['Embattled', ' 68%', 'Available Nov 20', 'data/img/poster-56.jpg'], ['Crock Of Gold: A Few Rounds With Shane MacGowan', ' 95%', 'Available Dec 4', 'data/img/poster-57.jpg'], ['Team Marco', ' 63%', 'Available Nov 20', 'data/img/poster-58.jpg'], ['Fatman', ' 46%', 'Available Nov 24', 'data/img/poster-59.jpg'], ['Jiu Jitsu', ' 30%', 'Available Nov 20', 'data/img/poster-60.jpg'], ['The Orange Years: The Nickelodeon Story', ' 86%', 'Available Nov 17', 'data/img/poster-61.jpg'], ['King Of Knives', ' 75%', 'Available Dec 1', 'data/img/poster-62.jpg'], ['The Mystery Of The Pink Flamingo', ' 100%', 'Available Dec 1', 'data/img/poster-63.jpg'], ['Truth Is The Only Client: The Official Investigation Of The Murder Of John F. Kennedy', ' 100%', 'Available Nov 17', 'data/img/poster-64.jpg']]\n"
     ]
    }
   ],
   "source": [
    "# This empty list will later contain my precious data\n",
    "moviesData = []\n",
    "# I am creating a counter, this will print the parser status and to generate filenames for images\n",
    "count = 0\n",
    "for movie in movies:\n",
    "    \n",
    "    count = count + 1\n",
    "    print(count)\n",
    "    # we can get precise elements by using css selectors, classes, tag names and xpaths\n",
    "    title = movie.find_element_by_class_name('movieTitle').text\n",
    "    rating = movie.find_element_by_class_name('tMeterIcon').text\n",
    "    available = movie.find_element_by_class_name('release-date').text\n",
    "    image = movie.find_element_by_tag_name('img')\n",
    "    \n",
    "    # it is also possible to get accessory information from collected elements, \n",
    "    # in this case we are interested in the src attribute\n",
    "    source = image.get_attribute(\"src\")\n",
    "    \n",
    "    # We use urllib to read the poster images and save them locally in a special folder. In order to avoid\n",
    "    # overwriting we use the {0}.format('dynamic variable') to save all of them\n",
    "    imgdestination = \"data/img/poster-{0}.jpg\".format(count)\n",
    "    urllib.request.urlretrieve(source, imgdestination)\n",
    "    \n",
    "    # We append a shorter list to the initial list\n",
    "    moviesData.append([title, rating, available, imgdestination])\n",
    "\n",
    "# Safety print to check everything is fine!\n",
    "print(moviesData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Move list of lists content to a pandas dataframe**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use pandas to create a dataframe holding the information we gathered from Rotten Tomatoes. This is the gateway to data analysis and data cleaning. From pandas we can export a `.csv` file that can later be cleaned, loaded in various applications and finally used to code our prototype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllStreamingTitles = pd.DataFrame(moviesData, columns=[\"title\", \"rating\", \"available\", \"img\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "      <th>available</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Shadow In The Cloud</td>\n",
       "      <td>76%</td>\n",
       "      <td>Available Jan 1</td>\n",
       "      <td>data/img/poster-10.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Sister Of The Groom</td>\n",
       "      <td>47%</td>\n",
       "      <td>Available Dec 18</td>\n",
       "      <td>data/img/poster-21.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I Am Lisa</td>\n",
       "      <td>91%</td>\n",
       "      <td>Available Jan 5</td>\n",
       "      <td>data/img/poster-11.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Hunter Hunter</td>\n",
       "      <td>93%</td>\n",
       "      <td>Available Dec 18</td>\n",
       "      <td>data/img/poster-17.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Parallel</td>\n",
       "      <td>80%</td>\n",
       "      <td>Available Dec 11</td>\n",
       "      <td>data/img/poster-1.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  title rating         available                     img\n",
       "9   Shadow In The Cloud    76%   Available Jan 1  data/img/poster-10.jpg\n",
       "20  Sister Of The Groom    47%  Available Dec 18  data/img/poster-21.jpg\n",
       "10            I Am Lisa    91%   Available Jan 5  data/img/poster-11.jpg\n",
       "16        Hunter Hunter    93%  Available Dec 18  data/img/poster-17.jpg\n",
       "0              Parallel    80%  Available Dec 11   data/img/poster-1.jpg"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Previewing 5 random rows from my dataframe\n",
    "AllStreamingTitles.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6: Export dataframe to csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the 'data' subfolder after executing this cell to see the file.\n",
    "df.to_csv('data/reviewsDataRaw.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping across multiple pages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we need to scrape more than the first page? Currently we are getting titles only from the initial screen. \n",
    "Instead we want to keep going: our script should be able to proceed down the page and do something everytime new titles need to be loaded. To approach this problem we need to go back to the source website. At the bottom of the page it's possible to spot a yellow \"Show More\" button, if we click on it new movies are loaded. \n",
    "\n",
    "**Positive aspects**\n",
    "- We have a distinct element, the yellow button with fairly easy interaction\n",
    "- We have a stable pattern and everything is happening on one single page, we don't need to touch the page URL\n",
    "- We can adopt a similar procedure to the previous one.\n",
    "\n",
    "**Negative aspects**\n",
    "- The page indefinitely loads new content, the button does not change and/or disappear at some point\n",
    "\n",
    "This calls for a fairly simple strategy. We will to modify only one step of our procedure: step 3. Instead of creating a variable with the first page results, we will create a while loop that keeps loading content up to a certain point, while getting all the movies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1 and 2 stay the same!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: automate clicking of the \"Show More\" button**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Websites generally don't like to be scraped (on some websites it's explicitly forbidden, so be careful). It can happen that if we are requesting too many pages or if we are interacting with the page too quickly we might get blocked or throttled (slowed). We will fake interactions with the \"Show More\" button every 10 seconds to avoid throttle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "while True:\n",
    "    # Generic asleep time to avoid throttle\n",
    "    time.sleep(10)\n",
    "    # Add 1 to counter so we can artificially stop the loop\n",
    "    counter = counter + 1 \n",
    "    if counter < 5:\n",
    "        # Find the button element and click on it to keep loading movies\n",
    "        driver.find_element_by_class_name('mb-load-btn').click()\n",
    "        # Dump all the elements in one list\n",
    "        moreMovies = driver.find_elements_by_class_name('mb-movie')\n",
    "    # It will set the condition to False if counter is greater than 5, this will break the while loop\n",
    "    if counter > 5:\n",
    "        break\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps 4, 5 are identical to the \"one page simple scraping\" example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ultimately, once we got everything we were interested in, we need to quit our WebDriver to avoid sessions piling up\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Each website is different, therefore scraping is often a tailored activity that requires some coding and strategizing effort. This tutorial presented some simple features and a couple of useful patterns if you need to scrape a relatively contained amount of data. There are more complex and articulated tasks that require more savviness, time and computing effort. One example are social media. For special or recurring cases there's a useful collection of tools that can be found [here](https://wiki.digitalmethods.net/Dmi/ToolDatabase)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. Guide to Selenium basic functions for scraping: https://www.scrapingbee.com/blog/selenium-python/\n",
    "2. Including WebDriver location in MacOS System Path: https://www.kenst.com/2015/03/including-the-chromedriver-location-in-macos-system-path/\n",
    "3. Data Science Skills Set: https://francesco-ai.medium.com/data-science-skills-list-9f38863adab5\n",
    "4. DMI Tools Database: https://wiki.digitalmethods.net/Dmi/ToolDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
