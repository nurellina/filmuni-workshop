{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Scraping with Selenium and Chrome webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a work in progress notebook. The idea is to create a basic tutorial to work with Selenium and Chrome Webdriver in order to scrape a dynamic website. I am also planning for a tutorial based on BeautifulSoup for static websites, the useful thing about Selenium is that it executes javascript when loading a request, this allows for dynamic websites to be scraped easily.\n",
    "\n",
    "Tutorial steps:\n",
    "- Importing Selenium and basic rules of webdrivers\n",
    "- First scraping: get all elements on one page\n",
    "- Create a temporized click function to keep scrolling down and repeat scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will make scraping possible\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "# Will allow to request, access and write urls of images and other links\n",
    "import urllib.request\n",
    "import requests\n",
    "\n",
    "# Will provide structure to data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Selenium and driver instance using Chrome WebDriver. To properly install ChromeDriver you can manually set the path following [this guide](https://www.kenst.com/2015/03/including-the-chromedriver-location-in-macos-system-path/) or use `pip install webdriver-manager` to manage it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Current google-chrome version is 87.0.4280\n",
      "[WDM] - Get LATEST driver version for 87.0.4280\n",
      "[WDM] - Driver [/Users/francescamorini/.wdm/drivers/chromedriver/mac64/87.0.4280.88/chromedriver] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "# We start an headless Chrome session. (Headless means that we don't open a visual browser window)\n",
    "options = Options()\n",
    "options.headless = True\n",
    "options.add_argument(\"--window-size=1920,1200\")\n",
    "\n",
    "# instead of setting path manually (still possible but a bit annoying) we use DriverManager\n",
    "driver = webdriver.Chrome(executable_path=ChromeDriverManager().install(), options=options)\n",
    "driver.get(\"https://www.rottentomatoes.com/browse/dvd-streaming-all\")\n",
    "\n",
    "# print(driver.page_source)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One time simple scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept behind scraping is quite easy to grasp. We are creating a script that behaves like a human. By using the correct methods we can fake human interactions on webpage, while usually scripts \"see\" different things than us, Selenium, Beautiful Soup and WebDrivers are built to see pages as we would do with our eyes. Scripts are obviously faster, they don't get to go to the toilet and they do everything very enthusiastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only getting first page results\n",
    "movies = driver.find_elements_by_class_name('mb-movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "[['Parallel', ' 80%', 'Available Dec 11', 'data/img/poster-1.jpg'], ['The Emoji Story (Picture Character)', ' 91%', 'Available Dec 22', 'data/img/poster-2.jpg'], ['Soul', ' 96%', 'Available Dec 25', 'data/img/poster-3.jpg'], ['Sing Me A Song', ' 88%', 'Available Jan 1', 'data/img/poster-4.jpg'], ['Pieces Of A Woman', ' 76%', 'Available Jan 7', 'data/img/poster-5.jpg'], [\"I'M Your Woman\", ' 81%', 'Available Dec 11', 'data/img/poster-6.jpg'], ['Lupin III: The First', ' 93%', 'Available Dec 15', 'data/img/poster-7.jpg'], ['Wander Darkly', ' 75%', 'Available Dec 11', 'data/img/poster-8.jpg'], [\"Sylvie'S Love\", ' 92%', 'Available Dec 23', 'data/img/poster-9.jpg'], ['Safety', ' 82%', 'Available Dec 11', 'data/img/poster-10.jpg'], ['Beasts Clawing At Straws', ' 96%', 'Available Dec 15', 'data/img/poster-11.jpg'], ['Wolfwalkers', ' 99%', 'Available Dec 11', 'data/img/poster-12.jpg'], ['Shadow In The Cloud', ' 77%', 'Available Jan 1', 'data/img/poster-13.jpg'], ['I Am Lisa', ' 91%', 'Available Jan 5', 'data/img/poster-14.jpg'], ['Shortcut', ' 56%', 'Available Dec 22', 'data/img/poster-15.jpg'], ['The Midnight Sky', ' 52%', 'Available Dec 23', 'data/img/poster-16.jpg'], [\"Ma Rainey'S Black Bottom\", ' 98%', 'Available Dec 18', 'data/img/poster-17.jpg'], ['Archenemy', ' 71%', 'Available Dec 11', 'data/img/poster-18.jpg'], ['Skylines', ' 52%', 'Available Dec 18', 'data/img/poster-19.jpg'], ['Hunter Hunter', ' 93%', 'Available Dec 18', 'data/img/poster-20.jpg'], ['The Prom', ' 58%', 'Available Dec 11', 'data/img/poster-21.jpg'], ['Wild Mountain Thyme', ' 27%', 'Available Dec 11', 'data/img/poster-22.jpg'], ['The Stand In', ' 29%', 'Available Dec 11', 'data/img/poster-23.jpg'], ['Sister Of The Groom', ' 44%', 'Available Dec 18', 'data/img/poster-24.jpg'], ['Max Cloud', ' 53%', 'Available Dec 18', 'data/img/poster-25.jpg'], ['Breach', ' 20%', 'Available Dec 18', 'data/img/poster-26.jpg'], ['We Can Be Heroes', ' 69%', 'Available Jan 1', 'data/img/poster-27.jpg'], ['Yearly Departed', ' 80%', 'Available Dec 30', 'data/img/poster-28.jpg'], ['Modern Persuasion', ' 53%', 'Available Dec 18', 'data/img/poster-29.jpg'], ['The One You Feed', ' 40%', 'Available Dec 29', 'data/img/poster-30.jpg'], ['If Not Now, When?', ' 44%', 'Available Jan 8', 'data/img/poster-31.jpg'], ['Ariana Grande: Excuse Me, I Love You', ' 50%', 'Available Dec 21', 'data/img/poster-32.jpg']]\n"
     ]
    }
   ],
   "source": [
    "# This empty list will later contain my precious data\n",
    "moviesData = []\n",
    "# I am creating a counter, this will print the parser status and to generate filenames for images\n",
    "count = 0\n",
    "for movie in movies:\n",
    "    \n",
    "    count = count + 1\n",
    "    print(count)\n",
    "    # we can get precise elements by using css selectors, classes, tag names and xpaths\n",
    "    title = movie.find_element_by_class_name('movieTitle').text\n",
    "    rating = movie.find_element_by_class_name('tMeterIcon').text\n",
    "    available = movie.find_element_by_class_name('release-date').text\n",
    "    image = movie.find_element_by_tag_name('img')\n",
    "    \n",
    "    # it is also possible to get accessory information from collected elements, \n",
    "    # in this case we are interested in the src attribute\n",
    "    source = image.get_attribute(\"src\")\n",
    "    \n",
    "    # We use urllib to read the poster images and save them locally in a special folder. In order to avoid\n",
    "    # overwriting we use the {0}.format('dynamic variable') to save all of them\n",
    "    imgdestination = \"data/img/poster-{0}.jpg\".format(count)\n",
    "    urllib.request.urlretrieve(source, imgdestination)\n",
    "    \n",
    "    # We append a shorter list to the initial list\n",
    "    moviesData.append([title, rating, available, imgdestination])\n",
    "\n",
    "# Safety print to check everything is fine!\n",
    "print(moviesData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use pandas to create a dataframe holding the information we gathered from Rotten Tomatoes. This is the gateway to data analysis and data cleaning. From pandas we can export a `.csv` file that can later be cleaned, loaded in various applications and finally used to code our prototype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllStreamingTitles = pd.DataFrame(moviesData, columns=[\"title\", \"rating\", \"available\", \"img\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "      <th>available</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>The One You Feed</td>\n",
       "      <td>40%</td>\n",
       "      <td>Available Dec 29</td>\n",
       "      <td>data/img/poster-30.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>If Not Now, When?</td>\n",
       "      <td>44%</td>\n",
       "      <td>Available Jan 8</td>\n",
       "      <td>data/img/poster-31.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>We Can Be Heroes</td>\n",
       "      <td>69%</td>\n",
       "      <td>Available Jan 1</td>\n",
       "      <td>data/img/poster-27.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I Am Lisa</td>\n",
       "      <td>91%</td>\n",
       "      <td>Available Jan 5</td>\n",
       "      <td>data/img/poster-14.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pieces Of A Woman</td>\n",
       "      <td>76%</td>\n",
       "      <td>Available Jan 7</td>\n",
       "      <td>data/img/poster-5.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                title rating         available                     img\n",
       "29   The One You Feed    40%  Available Dec 29  data/img/poster-30.jpg\n",
       "30  If Not Now, When?    44%   Available Jan 8  data/img/poster-31.jpg\n",
       "26   We Can Be Heroes    69%   Available Jan 1  data/img/poster-27.jpg\n",
       "13          I Am Lisa    91%   Available Jan 5  data/img/poster-14.jpg\n",
       "4   Pieces Of A Woman    76%   Available Jan 7   data/img/poster-5.jpg"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AllStreamingTitles.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the 'data' subfolder after executing this cell to see the file.\n",
    "df.to_csv('data/reviewsDataRaw.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous scraping across pages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we need to scrape more than the first page? Currently we are getting titles only from this initial screen. ![initial screen of rotten tomatoes with movies preview images, we see a limited amount of titles](screenshot.png)\n",
    "\n",
    "Instead we want to keep going: our script should be able to proceed down the page and do something everytime titles are not loading to load new information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#While loop with some kind of timer to temporize click and expand results?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPConnectionPool(host='127.0.0.1', port=56558): Max retries exceeded with url: /session/89d7045f827516940d122578e543205b/element (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd570ad31f0>: Failed to establish a new connection: [Errno 61] Connection refused'))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "moreMovies = showMore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all elements again\n",
    "movies = driver.find_elements_by_class_name('mb-movie')\n",
    "\n",
    "for movie in movies:\n",
    "    print(movie.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
